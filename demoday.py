# -*- coding: utf-8 -*-
"""DEMODAY.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/136-R_AwjOt3Hv0wNbSKgwxfb83SwWO4C

Library installation
"""

pip install pandas numpy matplotlib seaborn scikit-learn scipy

# 1. Install AWS CLI (quietly)
!pip install awscli --upgrade --quiet

# 2. Make a directory for the data
!mkdir -p /content/CSE-CIC-IDS2018

# 3. Download ONLY ONE specific file (safer than syncing the whole bucket)
# Target: Friday-02-03-2018_TrafficForML_CICFlowMeter.csv (~330MB)
print("‚è¨ Downloading Friday-02-03-2018 CSV... (this may take a minute)")
!aws s3 cp --no-sign-request --region us-east-1 \
    "s3://cse-cic-ids2018/Processed Traffic Data for ML Algorithms/Friday-02-03-2018_TrafficForML_CICFlowMeter.csv" \
    /content/CSE-CIC-IDS2018/Friday-02-03-2018_TrafficForML_CICFlowMeter.csv

# 4. Verify it exists
print("\n‚úÖ Verifying download:")
!ls -lh /content/CSE-CIC-IDS2018/

# 5. Test loading it with Pandas
import pandas as pd
file_path = "/content/CSE-CIC-IDS2018/Friday-02-03-2018_TrafficForML_CICFlowMeter.csv"

print("\nüìä Checking CSV structure...")
try:
    # Load just 5 rows to check columns
    df = pd.read_csv(file_path, nrows=5)
    print("Success! Columns found:", list(df.columns[:5]), "...")
    print("Total columns:", len(df.columns))
except FileNotFoundError:
    print("‚ùå Error: File not found. Download might have failed.")

import pandas as pd

# 1. Load your dataset (it currently HAS labels)
df = pd.read_csv(file_path)

# 2. Identify the label column name (e.g., 'Label', 'class', 'attack_type')
label_col = 'Label'  # Change this to whatever your label column is named

# 3. SEPARATE them: Keep labels aside for later testing, remove from training data
y_truth = df[label_col]             # Save answers for later validation
X_unlabeled = df.drop(columns=[label_col]) # THIS is your now "unlabeled" data

print("Original shape:", df.shape)
print("Unlabeled shape:", X_unlabeled.shape)

# Now you train ONLY on X_unlabeled
# kmeans.fit(X_unlabeled)

import pandas as pd
import numpy as np

# ===========================
# ‚öôÔ∏è CONFIGURATION
# ===========================
FILE_PATH = "/content/CSE-CIC-IDS2018/Friday-02-03-2018_TrafficForML_CICFlowMeter.csv"  # Ensure this path matches your downloaded file
N_ROWS = 50000 # Changed to import 1 million rows

# ===========================
# üßπ CLEANING SCRIPT
# ===========================
print(f"1Ô∏è‚É£ Loading top {N_ROWS} rows...")
try:
    df_raw = pd.read_csv(FILE_PATH, nrows=N_ROWS)
except FileNotFoundError:
    print("‚ùå Error: File not found. Please check the FILE_PATH.")
    df_raw = pd.DataFrame() # Empty DF to prevent crash later if file missing

if not df_raw.empty:
    print(f"   Original shape: {df_raw.shape}")

    # 2. Strip whitespace from column names (crucial for this dataset)
    df_raw.columns = df_raw.columns.str.strip()

    # 3. Separate and save Labels (Ground Truth) for later
    if 'Label' in df_raw.columns:
        y_labels = df_raw['Label']
        df_clean = df_raw.drop(columns=['Label'])
        print("   ‚úÖ Separated 'Label' column.")
    else:
        y_labels = None
        df_clean = df_raw
        print("   ‚ÑπÔ∏è 'Label' column not found (already unlabeled?).")

    # 4. Drop non-feature columns (IDs, IPs, Times)
    drop_cols = ['Timestamp', 'Flow ID', 'Src IP', 'Dst IP', 'Src Port', 'Dst Port', 'Protocol']
    df_clean = df_clean.drop(columns=[c for c in drop_cols if c in df_clean.columns], errors='ignore')

    # 5. "Clench": Replace Infinity with NaN, then drop NaNs
    df_clean = df_clean.replace([np.inf, -np.inf], np.nan)
    original_len = len(df_clean)
    df_clean = df_clean.dropna()
    dropped_count = original_len - len(df_clean)

    # 6. Ensure all remaining data is numeric
    df_clean = df_clean.apply(pd.to_numeric, errors='coerce').dropna()

    print("\n=== üßπ CLEANING RESULTS ===")
    print(f"Rows dropped (NaN/Inf): {dropped_count}")
    print(f"Final Cleaned Shape:    {df_clean.shape}")
    print("Data is now unlabeled and ready for scaling/training.")

    # Optional: Peek at the data
    print("\nFirst 5 rows of cleaned data:")
    print(df_clean.head())

from sklearn.preprocessing import RobustScaler
from sklearn.decomposition import PCA
import numpy as np

# Confirming input size
print(f"üì• Starting with {len(df_clean)} rows of cleaned data...")

# ===========================
# 2Ô∏è‚É£ SCALE DATA (ALL ROWS)
# ===========================
print("2Ô∏è‚É£ Scaling all data...")
robust = RobustScaler()
# This fits and transforms ALL N_ROWS
X_scaled = robust.fit_transform(df_clean)

# ===========================
# 3Ô∏è‚É£ PCA ‚Üí 2D (ALL ROWS)
# ===========================
print("3Ô∏è‚É£ Applying PCA to all data...")
pca = PCA(n_components=2)
# This reduces ALL N_ROWS to 2 dimensions
X_pca = pca.fit_transform(X_scaled)

print("\n=== ‚úÖ STATUS REPORT ===")
print(f"Original Rows Processed: {len(df_clean)}")
print(f"Final 2D Data Shape:     {X_pca.shape}")

from sklearn.cluster import KMeans
import numpy as np
import joblib

# ===========================
# 4Ô∏è‚É£ K-MEANS CLUSTERING
# ===========================
print(f"4Ô∏è‚É£ Training HYBRID CLUSTERING on {len(X_pca)} rows...")

kmeans = KMeans(n_clusters=2, random_state=42, n_init=10)

# Fit model and predict clusters
train_clusters = kmeans.fit_predict(X_pca)
centroids = kmeans.cluster_centers_

# --- RESULTS ---
print("\n=== ‚úÖ CLUSTERING COMPLETE ===")
counts = np.bincount(train_clusters)
print(f"Cluster 0 size: {counts[0]} points")
print(f"Cluster 1 size: {counts[1]} points")
print(f"Centroid locations:\n {centroids}")

# ===========================
# 5Ô∏è‚É£ SAVE TRAINED MODELS
# ===========================
print("\nüíæ Saving model files...")
joblib.dump(robust, "robust.pkl")   # scaler
joblib.dump(pca, "pca.pkl")         # pca transformer
joblib.dump(kmeans, "kmeans.pkl")   # trained kmeans model
print("‚úÖ Saved: robust.pkl, pca.pkl, kmeans.pkl")

train_clusters = kmeans.fit_predict(X_pca)
centroids = kmeans.cluster_centers_

print(f"Centroid locations:\n {centroids}")

# After scaling
robust = RobustScaler()
X_scaled = robust.fit_transform(df_clean)
joblib.dump(robust, "robust.pkl")

# After PCA
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X_scaled)
joblib.dump(pca, "pca.pkl")

# After KMeans
kmeans = KMeans(n_clusters=2, random_state=42, n_init=10)
train_clusters = kmeans.fit_predict(X_pca)
centroids = kmeans.cluster_centers_
joblib.dump(kmeans, "kmeans.pkl")

print("‚úÖ Saved robust.pkl, pca.pkl, kmeans.pkl")

from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans
import matplotlib.pyplot as plt
import numpy as np

# Assuming 'X' is your original data - REMOVING REDUNDANT STEPS
# 1Ô∏è‚É£ SCALING - Already done in previous cell, using X_scaled
# scaler = StandardScaler()
# X_scaled = scaler.fit_transform(X)

# 2Ô∏è‚É£ PCA - Already done in previous cell, using X_pca
# pca = PCA(n_components=2)
# X_pca = pca.fit_transform(X_scaled)

# 3Ô∏è‚É£ K-MEANS - Already done in previous cell, using train_clusters and centroids
# kmeans = KMeans(n_clusters=2, random_state=42)
# train_clusters = kmeans.fit_predict(X_pca)
# centroids = kmeans.cluster_centers_

# Use the results from previous cells: X_pca, train_clusters, centroids

# 4Ô∏è‚É£ VISUALIZE CLUSTERS
print("4Ô∏è‚É£ Generating Cluster Plot using pre-calculated PCA and clusters...")

plt.figure(figsize=(10, 8))

# Define colors for clusters
colors = ['#1f77b4', '#ff7f0e'] # Blue and Orange
labels = ['Cluster 0', 'Cluster 1']

# Plot each cluster's points
for i in [0, 1]:
    # Select points belonging to this cluster
    # Use the train_clusters variable from the previous K-Means cell
    points = X_pca[train_clusters == i]
    plt.scatter(points[:, 0], points[:, 1], s=10, alpha=0.6, c=colors[i], label=labels[i])

# Plot the centroids (the 'center' of each cluster)
# Use the centroids variable from the previous K-Means cell
plt.scatter(centroids[:, 0], centroids[:, 1], c='black', marker='X', s=300, linewidth=2, label='Centroids')

plt.title(f"HYBRID Clustering Results (N={len(X_pca)})", fontsize=14)
plt.xlabel("Principal Component 1")
plt.ylabel("Principal Component 2")
plt.legend()
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.show()

import pandas as pd
import numpy as np
import joblib
from scipy import stats
import os

# ===========================
# ‚öô CONFIGURATION
# ===========================
DATASET_PATH = "/content/CSE-CIC-IDS2018/Friday-02-03-2018_TrafficForML_CICFlowMeter.csv"
MODEL_DIR = "/content/"
OUTPUT_CONFIG_FILE = "cluster_config.pkl"
SAMPLES_TO_TAKE = 20 # Increased to 20 to get a better average

# ===========================
# üõ† HELPER (Same as before)
# ===========================
def preprocess_for_inference(df_raw, scaler_model):
    df_proc = df_raw.copy()
    df_proc.columns = df_proc.columns.str.strip()
    drop_cols = ['Label', 'Timestamp', 'Flow ID', 'Src IP', 'Dst IP', 'Src Port', 'Dst Port', 'Protocol']
    df_proc = df_proc.drop(columns=[c for c in drop_cols if c in df_proc.columns], errors='ignore')
    df_proc = df_proc.replace([np.inf, -np.inf], np.nan).dropna()
    df_proc = df_proc.apply(pd.to_numeric, errors='coerce').dropna()
    if hasattr(scaler_model, 'feature_names_in_'):
        required_cols = scaler_model.feature_names_in_
        df_proc = df_proc[required_cols]
    return df_proc

# ===========================
# üöÄ MAIN EXECUTION
# ===========================
try:
    print("1. Loading models...")
    robust = joblib.load(os.path.join(MODEL_DIR, "robust.pkl"))
    pca = joblib.load(os.path.join(MODEL_DIR, "pca.pkl"))
    kmeans = joblib.load(os.path.join(MODEL_DIR, "kmeans.pkl"))

    print("2. Loading Data...")
    df = pd.read_csv(DATASET_PATH)
    df.columns = df.columns.str.strip()

    # --- KEY CHANGE: Trust the 'Benign' samples most ---
    print("3. Calibrating based on Normal Traffic...")

    # Get 20 Normal samples
    normal_samples = df[df['Label'] == 'Benign'].sample(n=SAMPLES_TO_TAKE, random_state=42)

    # Predict
    X_norm = preprocess_for_inference(normal_samples, robust)
    X_norm_pca = pca.transform(robust.transform(X_norm))
    norm_preds = kmeans.predict(X_norm_pca)

    # Identify Normal Cluster
    NORMAL_CLUSTER_ID = int(stats.mode(norm_preds, keepdims=True)[0][0])

    # --- LOGIC FIX: Process of Elimination ---
    # If Normal is 0, Attack is 1. If Normal is 1, Attack is 0.
    ATTACK_CLUSTER_ID = 1 - NORMAL_CLUSTER_ID

    print(f"   üëâ Normal Samples are consistently Cluster {NORMAL_CLUSTER_ID}")
    print(f"   üëâ Therefore, Attack Cluster is set to {ATTACK_CLUSTER_ID}")

    # Save
    config = {
        "attack_cluster": ATTACK_CLUSTER_ID,
        "normal_cluster": NORMAL_CLUSTER_ID,
        "description": "Derived by finding Normal cluster and assigning Attack to the opposite."
    }

    joblib.dump(config, OUTPUT_CONFIG_FILE)
    print(f"\n‚úÖ Configuration saved to '{OUTPUT_CONFIG_FILE}'")
    print(config)

except Exception as e:
    print(f"‚ùå Error: {e}")

import numpy as np
import matplotlib.pyplot as plt
from scipy.spatial import ConvexHull

def get_inflated_hull(points, padding=0.5):
    """
    Creates a convex hull boundary that is pushed outward by 'padding'
    so it doesn't cut through the points.
    """
    if len(points) < 3:
        return None # Cannot make a hull with < 3 points

    # 1. Calculate the standard Convex Hull
    hull = ConvexHull(points)

    # 2. Get the centroid of the cluster
    centroid = np.mean(points, axis=0)

    # 3. Get the vertices of the hull (in counter-clockwise order)
    # hull.vertices gives indices of points forming the hull
    hull_indices = hull.vertices
    hull_pts = points[hull_indices]

    # 4. Inflate the hull
    # Calculate vector from centroid to each hull point
    vecs = hull_pts - centroid

    # Normalize these vectors (make them length 1)
    # We add a tiny epsilon to avoid division by zero
    norms = np.linalg.norm(vecs, axis=1, keepdims=True)
    unit_vecs = vecs / (norms + 1e-9)

    # Move the points outward by the padding amount
    inflated_pts = hull_pts + (unit_vecs * padding)

    # 5. Close the loop for plotting (append the first point to the end)
    boundary_line = np.vstack([inflated_pts, inflated_pts[0]])

    return boundary_line

# --- PLOTTING ---

# 1. Define your colors and padding
colors = ['blue', 'orange']
padding_amount = 10.0  # <--- Adjust this to move boundary further out

plt.figure(figsize=(10, 8))

# 2. Iterate through clusters
for cid in np.unique(train_clusters):
    # Get points for this cluster
    pts = X_pca[train_clusters == cid]

    # Plot the scatter points
    plt.scatter(pts[:, 0], pts[:, 1], c=colors[cid], label=f'Cluster {cid}', alpha=0.6)

    # Calculate and plot the boundary
    boundary = get_inflated_hull(pts, padding=padding_amount)

    if boundary is not None:
        # Plot the boundary line (Note: we plot the inflated boundary, not the hull simplices)
        plt.plot(boundary[:, 0], boundary[:, 1], color=colors[cid], linestyle='-', linewidth=2)

        # Optional: Add a shaded fill
        plt.fill(boundary[:, 0], boundary[:, 1], color=colors[cid], alpha=0.1)

# Plot Centroids
plt.scatter(centroids[:, 0], centroids[:, 1], c='black', marker='X', s=200, label='Centroids', zorder=10)

plt.legend()
plt.title(f"Cluster Boundaries with {padding_amount} Padding")
plt.show()

import joblib

def compute_and_save_boundaries(X_pca, train_clusters, padding_amount=10.0):
    boundaries = {}

    for cid in np.unique(train_clusters):
        pts = X_pca[train_clusters == cid]
        boundary = get_inflated_hull(pts, padding=padding_amount)
        boundaries[int(cid)] = boundary  # store it

    # Save boundaries.pkl
    joblib.dump(boundaries, "boundaries.pkl")
    print("‚úÖ boundaries.pkl saved successfully!")

    return boundaries

boundaries = compute_and_save_boundaries(X_pca, train_clusters, padding_amount=10.0)

import joblib
import numpy as np
from shapely.geometry import Point, Polygon

# Load models
robust = joblib.load("robust.pkl")
cluster = joblib.load("cluster_config.pkl");
pca = joblib.load("pca.pkl")
kmeans = joblib.load("kmeans.pkl")
boundaries = joblib.load("boundaries.pkl")

import numpy as np
from shapely.geometry import Point, Polygon

def classify_input(raw_row):
    """
    raw_row ‚Üí a single row list/array of feature values (same order as training)
    """

    # ---------------------------------
    # 1Ô∏è‚É£ Scale input
    # ---------------------------------
    scaled = robust.transform([raw_row])

    # ---------------------------------
    # 2Ô∏è‚É£ PCA transform
    # ---------------------------------
    pca_point = pca.transform(scaled)[0]

    # ---------------------------------
    # 3Ô∏è‚É£ KMeans cluster prediction
    # ---------------------------------
    predicted_cluster = int(kmeans.predict([pca_point])[0])

    # ---------------------------------
    # 4Ô∏è‚É£ Load attack/normal cluster IDs
    # ---------------------------------
    attack_cluster = cluster["attack_cluster"]
    normal_cluster = cluster["normal_cluster"]

    # ---------------------------------
    # 5Ô∏è‚É£ Get boundary of predicted cluster
    # ---------------------------------
    boundary_points = boundaries.get(predicted_cluster)

    if boundary_points is None:
        return {
            "cluster": predicted_cluster,
            "classification": "UNKNOWN",
            "message": "No boundary found for this cluster."
        }

    poly = Polygon(boundary_points)
    inside = poly.contains(Point(pca_point))

    # ---------------------------------
    # 6Ô∏è‚É£ Decision Logic
    # ---------------------------------

    if inside:
        if predicted_cluster == normal_cluster:
            result = "NORMAL"
        else:
            result = "KNOWN ATTACK"
    else:
        # Point is outside ‚Üí Zero-day anomaly
        result = "ZERO-DAY ATTACK"

    return {
        "cluster": predicted_cluster,
        "inside_boundary": inside,
        "classification": result
    }

def classify_input(raw_row):
    # Convert row to DataFrame with correct feature names
    df = pd.DataFrame([raw_row], columns=robust.feature_names_in_)

    scaled = robust.transform(df)
    pca_point = pca.transform(scaled)[0]
    predicted_cluster = int(kmeans.predict([pca_point])[0])

    # Boundary logic
    boundary_points = boundaries.get(predicted_cluster)
    inside = Polygon(boundary_points).contains(Point(pca_point))

    if inside and predicted_cluster == cluster["normal_cluster"]:
        result = "NORMAL"
    elif inside and predicted_cluster == cluster["attack_cluster"]:
        result = "KNOWN ATTACK"
    else:
        result = "ZERO-DAY ATTACK"

    return {
        "cluster": predicted_cluster,
        "inside_boundary": inside,
        "classification": result
    }

test_row = df_clean.iloc[10].values   # or any new incoming data

output = classify_input(test_row)
print(output)

!pip install fastapi uvicorn pyngrok shapely joblib

# Commented out IPython magic to ensure Python compatibility.
# %%writefile app.py
# from fastapi import FastAPI
# from pydantic import BaseModel
# import joblib
# import numpy as np
# from shapely.geometry import Point, Polygon
# import pandas as pd
# 
# app = FastAPI(title="Zero-Day Attack Detection API")
# 
# # Load Models
# robust = joblib.load("robust.pkl")
# pca = joblib.load("pca.pkl")
# kmeans = joblib.load("kmeans.pkl")
# boundaries = joblib.load("boundaries.pkl")
# cluster = joblib.load("cluster_config.pkl")  # attack & normal clusters
# 
# NORMAL_CLUSTER = cluster["normal_cluster"]
# ATTACK_CLUSTER = cluster["attack_cluster"]
# 
# class InputData(BaseModel):
#     features: list
# 
# def classify_input(raw_row):
#     df = pd.DataFrame([raw_row], columns=robust.feature_names_in_)
#     scaled = robust.transform(df)
#     pca_point = pca.transform(scaled)[0]
#     predicted_cluster = int(kmeans.predict([pca_point])[0])
# 
#     boundary_points = boundaries.get(predicted_cluster)
#     polygon = Polygon(boundary_points)
#     inside = polygon.contains(Point(pca_point))
# 
#     if inside:
#         if predicted_cluster == NORMAL_CLUSTER:
#             result = "NORMAL"
#         else:
#             result = "KNOWN ATTACK"
#     else:
#         result = "ZERO-DAY ATTACK"
# 
#     return {
#         "cluster": predicted_cluster,
#         "inside_boundary": inside,
#         "classification": result,
#         "pca_x": float(pca_point[0]),
#         "pca_y": float(pca_point[1])
#     }
# 
# @app.post("/predict")
# def predict(data: InputData):
#     return classify_input(data.features)
# 
# @app.get("/")
# def home():
#     return {"message": "Zero-Day Attack Detection API is running in Colab!"}
#

!wget https://github.com/cloudflare/cloudflared/releases/latest/download/cloudflared-linux-amd64 -O cloudflared
!chmod +x cloudflared

# Commented out IPython magic to ensure Python compatibility.
# %%writefile app.py
# from fastapi import FastAPI
# from pydantic import BaseModel
# import joblib
# import numpy as np
# from shapely.geometry import Point, Polygon
# import pandas as pd
# 
# app = FastAPI(title="Zero-Day Attack Detection API")
# 
# # Load Models
# robust = joblib.load("robust.pkl")
# pca = joblib.load("pca.pkl")
# kmeans = joblib.load("kmeans.pkl")
# boundaries = joblib.load("boundaries.pkl")
# cluster = joblib.load("cluster_config.pkl")
# 
# NORMAL_CLUSTER = cluster["normal_cluster"]
# ATTACK_CLUSTER = cluster["attack_cluster"]
# 
# class InputData(BaseModel):
#     features: list
# 
# def classify_input(raw_row):
#     df = pd.DataFrame([raw_row], columns=robust.feature_names_in_)
#     scaled = robust.transform(df)
#     pca_point = pca.transform(scaled)[0]
#     predicted_cluster = int(kmeans.predict([pca_point])[0])
# 
#     boundary_points = boundaries.get(predicted_cluster)
#     polygon = Polygon(boundary_points)
#     inside = polygon.contains(Point(pca_point))
# 
#     if inside:
#         if predicted_cluster == NORMAL_CLUSTER:
#             result = "NORMAL"
#         else:
#             result = "KNOWN ATTACK"
#     else:
#         result = "ZERO-DAY ATTACK"
# 
#     return {
#         "cluster": predicted_cluster,
#         "inside_boundary": inside,
#         "classification": result,
#         "pca_x": float(pca_point[0]),
#         "pca_y": float(pca_point[1])
#     }
# 
# @app.post("/predict")
# def predict(data: InputData):
#     return classify_input(data.features)
# 
# @app.get("/")
# def home():
#     return {"message": "Zero-Day Attack Detection API is running with Cloudflare!"}
#

!nohup uvicorn app:app --host 0.0.0.0 --port 8000 --reload &

!./cloudflared tunnel --url http://127.0.0.1:8000 --no-autoupdate

len(robust.feature_names_in_)